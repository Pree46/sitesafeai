{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9319e2",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9d512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "camel-ai 0.2.18 requires numpy<2,>=1, but you have numpy 2.2.6 which is incompatible.\n",
      "langchain 0.3.15 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-community 0.3.15 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "openvino 2024.1.0 requires numpy<2.0.0,>=1.16.6, but you have numpy 2.2.6 which is incompatible.\n",
      "patchify 0.2.3 requires numpy<2,>=1, but you have numpy 2.2.6 which is incompatible.\n",
      "streamlit 1.41.1 requires pillow<12,>=7.1.0, but you have pillow 12.0.0 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.2.6 which is incompatible.\n",
      "datasets 2.12.0 requires dill<0.3.7,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.2.6 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch torchvision opencv-python pillow numpy matplotlib tqdm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7014fbc",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78afaf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "Torchvision version: 0.24.1+cpu\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdaca3",
   "metadata": {},
   "source": [
    "## Step 3: Setup Dataset and Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b03d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety Classes (10):\n",
      "  0: Hardhat\n",
      "  1: Mask\n",
      "  2: NO-Hardhat\n",
      "  3: NO-Mask\n",
      "  4: NO-Safety Vest\n",
      "  5: Person\n",
      "  6: Safety Cone\n",
      "  7: Safety Vest\n",
      "  8: machinery\n",
      "  9: vehicle\n",
      "\n",
      "Total classes for training: 11 (including background)\n",
      "\n",
      "✓ Dataset structure verified\n"
     ]
    }
   ],
   "source": [
    "# Your safety dataset configuration\n",
    "DATASETS_DIR = Path(\"./datasets\")\n",
    "OUT_DIR = DATASETS_DIR\n",
    "CFG_PATH = OUT_DIR / \"data.yaml\"\n",
    "\n",
    "# Safety equipment class names\n",
    "SAFETY_CLASSES = {\n",
    "    0: 'Hardhat',\n",
    "    1: 'Mask',\n",
    "    2: 'NO-Hardhat',\n",
    "    3: 'NO-Mask',\n",
    "    4: 'NO-Safety Vest',\n",
    "    5: 'Person',\n",
    "    6: 'Safety Cone',\n",
    "    7: 'Safety Vest',\n",
    "    8: 'machinery',\n",
    "    9: 'vehicle'\n",
    "}\n",
    "\n",
    "NUM_SAFETY_CLASSES = len(SAFETY_CLASSES)\n",
    "NUM_CLASSES_WITH_BG = NUM_SAFETY_CLASSES + 1  # +1 for background\n",
    "\n",
    "print(f\"Safety Classes ({NUM_SAFETY_CLASSES}):\")\n",
    "for idx, name in SAFETY_CLASSES.items():\n",
    "    print(f\"  {idx}: {name}\")\n",
    "print(f\"\\nTotal classes for training: {NUM_CLASSES_WITH_BG} (including background)\")\n",
    "\n",
    "# Verify dataset structure\n",
    "assert (CFG_PATH).exists(), f\"Config file not found: {CFG_PATH}\"\n",
    "assert (OUT_DIR / \"train\" / \"images\").exists(), \"Train images folder missing!\"\n",
    "assert (OUT_DIR / \"valid\" / \"images\").exists(), \"Validation images folder missing!\"\n",
    "print(\"\\n✓ Dataset structure verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d396c",
   "metadata": {},
   "source": [
    "## Step 4: Create Custom Dataset for Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a597f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training dataset: 2605 images\n",
      "✓ Validation dataset: 114 images\n",
      "\n",
      "✓ DataLoaders created\n",
      "  Train batches: 652 (batch_size=4)\n",
      "  Val batches: 29 (batch_size=4)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class SafetyDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for safety equipment detection\n",
    "    Expects YOLO format labels (converted to bounding boxes)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, label_dir, transforms=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Get all image files\n",
    "        self.images = sorted([f for f in os.listdir(img_dir) \n",
    "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = self.img_dir / img_name\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        img_width, img_height = image.size\n",
    "        \n",
    "        # Parse YOLO annotations\n",
    "        label_file = self.label_dir / img_name.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if label_file.exists():\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center = float(parts[1]) * img_width\n",
    "                        y_center = float(parts[2]) * img_height\n",
    "                        width = float(parts[3]) * img_width\n",
    "                        height = float(parts[4]) * img_height\n",
    "                        \n",
    "                        # Convert to xyxy format\n",
    "                        x1 = x_center - width / 2\n",
    "                        y1 = y_center - height / 2\n",
    "                        x2 = x_center + width / 2\n",
    "                        y2 = y_center + height / 2\n",
    "                        \n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(class_id + 1)  # +1 because 0 is reserved for background\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_tensor = T.ToTensor()(image)\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.tensor([]),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return image_tensor, target\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SafetyDetectionDataset(\n",
    "    str(OUT_DIR / \"train\" / \"images\"),\n",
    "    str(OUT_DIR / \"train\" / \"labels\")\n",
    ")\n",
    "\n",
    "val_dataset = SafetyDetectionDataset(\n",
    "    str(OUT_DIR / \"valid\" / \"images\"),\n",
    "    str(OUT_DIR / \"valid\" / \"labels\")\n",
    ")\n",
    "\n",
    "print(f\"✓ Training dataset: {len(train_dataset)} images\")\n",
    "print(f\"✓ Validation dataset: {len(val_dataset)} images\")\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)} (batch_size=4)\")\n",
    "print(f\"  Val batches: {len(val_loader)} (batch_size=4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c1ec3",
   "metadata": {},
   "source": [
    "## Step 5: Create and Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94f8d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preethi.R\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preethi.R\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL CONFIGURATION FOR TRANSFER LEARNING\n",
      "======================================================================\n",
      "\n",
      "Frozen layers (keeping ImageNet weights):\n",
      "  ✓ Backbone (ResNet50)\n",
      "  ✓ RPN (Region Proposal Network)\n",
      "\n",
      "Trainable layers (will be fine-tuned):\n",
      "  • ROI Head (box classifier)\n",
      "  • Box Predictor (for 11 classes: 10 safety + background)\n",
      "\n",
      "Total parameters: 41,345,286\n",
      "Trainable parameters: 13,952,055\n",
      "Frozen parameters: 27,393,231\n",
      "Trainable ratio: 33.75%\n",
      "\n",
      "✓ Model moved to cpu\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Load a new ResNet model (not using the previous COCO-trained one)\n",
    "finetuned_model = fasterrcnn_resnet50_fpn(pretrained=True, num_classes=91)\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = finetuned_model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one for safety classes\n",
    "# num_classes = 10 safety classes + 1 background\n",
    "finetuned_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES_WITH_BG)\n",
    "\n",
    "# Freeze backbone layers for transfer learning\n",
    "for name, param in finetuned_model.backbone.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze RPN\n",
    "for param in finetuned_model.rpn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train the head\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL CONFIGURATION FOR TRANSFER LEARNING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFrozen layers (keeping ImageNet weights):\")\n",
    "print(\"  ✓ Backbone (ResNet50)\")\n",
    "print(\"  ✓ RPN (Region Proposal Network)\")\n",
    "print(\"\\nTrainable layers (will be fine-tuned):\")\n",
    "print(\"  • ROI Head (box classifier)\")\n",
    "print(\"  • Box Predictor (for 11 classes: 10 safety + background)\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in finetuned_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "finetuned_model.to(device)\n",
    "print(f\"\\n✓ Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff21958",
   "metadata": {},
   "source": [
    "## Step 6: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf05400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, targets in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        predictions = model(images)\n",
    "        \n",
    "        for pred in predictions:\n",
    "            all_boxes.append(pred['boxes'].cpu().numpy())\n",
    "            all_scores.append(pred['scores'].cpu().numpy())\n",
    "            all_labels.append(pred['labels'].cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'boxes': all_boxes,\n",
    "        'scores': all_scores,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247350e",
   "metadata": {},
   "source": [
    "## Step 7: Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da35d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINE-TUNING CONFIGURATION\n",
      "======================================================================\n",
      "Epochs: 10\n",
      "Learning Rate: 0.001\n",
      "Optimizer: Adam\n",
      "Device: cpu\n",
      "Train samples: 2605\n",
      "Val samples: 114\n",
      "======================================================================\n",
      "\n",
      "Starting fine-tuning...\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b2d0fba8554dfdb8189cce022ecee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 10  # Start with 10 epochs, can increase if needed\n",
    "LEARNING_RATE = 0.001  # Lower LR for fine-tuning\n",
    "WARMUP_EPOCHS = 2\n",
    "\n",
    "# Setup optimizer (only for trainable parameters)\n",
    "params_to_optimize = [p for p in finetuned_model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(params_to_optimize, lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINE-TUNING CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "print(\"\\nStarting fine-tuning...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(finetuned_model, train_loader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Evaluating on validation set...\")\n",
    "        val_results = evaluate(finetuned_model, val_loader, device)\n",
    "        print(f\"Evaluation completed. Found predictions.\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINE-TUNING COMPLETED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0b6a7",
   "metadata": {},
   "source": [
    "## Step 8: Save and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for fine-tuned models\n",
    "finetuned_models_dir = Path(\"./resnet_finetuned_safety\")\n",
    "finetuned_models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "finetuned_model_path = finetuned_models_dir / \"finetuned_resnet_safety.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': finetuned_model.state_dict(),\n",
    "    'num_classes': NUM_CLASSES_WITH_BG,\n",
    "    'class_names': SAFETY_CLASSES,\n",
    "    'epoch': NUM_EPOCHS\n",
    "}, str(finetuned_model_path))\n",
    "\n",
    "print(f\"✓ Fine-tuned model saved to: {finetuned_model_path}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, marker='o', linewidth=2, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ResNet50 Fine-tuning on Safety Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Loss Reduction: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6d851",
   "metadata": {},
   "source": [
    "## Step 9: Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_finetuned_model(image_path, model, score_threshold=0.5):\n",
    "    \"\"\"Test fine-tuned model on single image\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = T.ToTensor()(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model([image_tensor])\n",
    "    \n",
    "    # Draw detections\n",
    "    img_array = np.array(image)\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    \n",
    "    detected_count = 0\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        if score > score_threshold:\n",
    "            detected_count += 1\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2 = min(img_array.shape[1], x2)\n",
    "            y2 = min(img_array.shape[0], y2)\n",
    "            \n",
    "            # Map label to class name (subtract 1 because 0 is background)\n",
    "            class_id = int(label) - 1\n",
    "            class_name = SAFETY_CLASSES.get(class_id, f\"Unknown_{class_id}\")\n",
    "            \n",
    "            # Draw box\n",
    "            cv2.rectangle(img_array, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw label\n",
    "            text = f\"{class_name}: {score:.2f}\"\n",
    "            cv2.putText(img_array, text, (x1, y1 - 5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    \n",
    "    return Image.fromarray(img_array), detected_count\n",
    "\n",
    "# Test on sample validation images\n",
    "test_images = list(Path(\"./datasets/valid/images\").glob(\"*.jpg\"))[:3]\n",
    "\n",
    "if test_images:\n",
    "    print(\"Testing fine-tuned model on sample images...\\n\")\n",
    "    for img_path in test_images:\n",
    "        result_image, num_detected = test_finetuned_model(str(img_path), finetuned_model)\n",
    "        print(f\"{img_path.name}: {num_detected} safety objects detected\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(result_image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{img_path.name} - {num_detected} detections\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19a53d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ Fine-tuned ResNet50 model on safety equipment dataset  \n",
    "✓ Transfer learning approach: froze backbone, trained head  \n",
    "✓ Model detects 10 safety classes  \n",
    "✓ Ready for quantization and OpenVINO deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
